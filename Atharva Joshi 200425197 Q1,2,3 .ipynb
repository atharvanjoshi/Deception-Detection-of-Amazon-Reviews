{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReview(reviewLine):\n",
    "    if reviewLine[1]=='__label2__':\n",
    "        reviewLine[1]=realLabel\n",
    "    else:\n",
    "        reviewLine[1]=fakeLabel\n",
    "    return (reviewLine[0], reviewLine[8], reviewLine[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DocID of any record is at position 0 in the dataset. Since it is the very first data to be returned, we have written reviewLine[0]. Similarly for Review Text and Label (Output) at positions 8 and 1 respectively. Here we assigned \"\\_\\_label2\\_\\_\" as \"Real\" and \"\\_\\_label1\\_\\_\" as \"Fake\" for easy understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import re,nltk\n",
    "def preProcess(text):\n",
    "    \n",
    "    #Convert to lower case\n",
    "    text= text.lower()\n",
    "    \n",
    "    #Tokenisation\n",
    "    tokens=text.split(' ')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation is the process of converting the text into individual entities called tokens. Here we split the text on the blank space. eg. \"I am Happy\" will be converted [\"I\",\"am\",\"Happy\"]. Here we have indulged in some simple normalization like converting all words to lower case. This is done because:\n",
    "### eg. {\"I am Happy\"}, {\"I am happy\"}. These two sentences will be tokenised as ['I','am','Happy','happy']. Here we know that Happy=happy. Hence to avoid this we convert every letter in the text to lower form.\n",
    "### However still there are some problems such as unnecessary tokens with this which are addressed and corrected in Q4 and 5 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    featureDictLocal={}\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            featureDict[t] += 1\n",
    "            featureDictLocal[t] += 1\n",
    "        except KeyError:            \n",
    "            featureDict[t] = 1\n",
    "            featureDictLocal[t] = 1\n",
    "    return featureDictLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we try to assign weights to each token. In this case the weight is equivalent to the frequency of the word in text. eg. For \"I am happy\" and \"I am sad\", the feature Dictionary will be {'I':2,'am':2,'happy':1,'sad':1}. There are various ways to assign weights more of which are explored in Q4 and 5 notebook. We are maintaining two dictionaries - Local and Global. Local dictionary stores the keys and weights for a individual text in the dataset. Global stores the keys and weights (counts) for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "crossValidationActual=[]\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    accuracySum = 0\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        crossValidationTestData = dataset[i:i+foldSize]\n",
    "        crossValidationTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(crossValidationTrainData)\n",
    "        crossValidationActual = [x[1] for x in crossValidationTestData]\n",
    "        crossValidationPredictedLabels = predictLabels(crossValidationTestData,classifier)\n",
    "        cv_results.append(precision_recall_fscore_support(crossValidationActual, crossValidationPredictedLabels, average='weighted'))\n",
    "        accuracySum+=(accuracy_score(crossValidationActual, crossValidationPredictedLabels))\n",
    "    print('Average Accuracy:%f' % (accuracySum/10))\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this we use KFold cross validation. Instead of using the libraries we have manually implemented it. Here we are using 10 fold cross validation. So folds=10 in the parameters. Here foldSize is the step size for the 'for loop'. The 'for loop' begins with the assigning of test data. In the first iteration the test data will be from the first record to the record at foldSize position. eg. Assume there are 100 records with 10 folds. foldSize will be 100/10=10. Hence, the test data will be from 0 to 9. Remaining will be the training data. Using this training data we train the classifier. We then use that classifier to predict labels for the test data extracted. We already have the actual labels in the second column of the test data. We extract them and compare that with the predicted lables to get Precision, Recall, FScore and Accuracy. We then continue the loop considering the second fold i.e 10 to 19 as the testing data and remaining (0 to 9 and 20 to 99) as the training data. The same process continues till the classifier is trained using the remaining data (0 to 89) and tested using the last fold i.e. 90 to 99 . Lastly we print the average accuracy of all 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "78556\n",
      "Training Classifier...\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Training Classifier...\n",
      "Average Accuracy:0.635893\n",
      "Fold 1: \n",
      "Precision: 0.624409\tRecall: 0.624405\tF Score:0.624407\n",
      "Fold 2: \n",
      "Precision: 0.665350\tRecall: 0.664881\tF Score:0.664827\n",
      "Fold 3: \n",
      "Precision: 0.623044\tRecall: 0.622024\tF Score:0.622004\n",
      "Fold 4: \n",
      "Precision: 0.641300\tRecall: 0.641071\tF Score:0.641020\n",
      "Fold 5: \n",
      "Precision: 0.631626\tRecall: 0.631548\tF Score:0.631210\n",
      "Fold 6: \n",
      "Precision: 0.635373\tRecall: 0.635119\tF Score:0.635208\n",
      "Fold 7: \n",
      "Precision: 0.629762\tRecall: 0.629762\tF Score:0.629762\n",
      "Fold 8: \n",
      "Precision: 0.631277\tRecall: 0.630952\tF Score:0.630913\n",
      "Fold 9: \n",
      "Precision: 0.664415\tRecall: 0.663095\tF Score:0.663069\n",
      "Fold 10: \n",
      "Precision: 0.616485\tRecall: 0.616071\tF Score:0.616119\n",
      "Average FScore:0.635854\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "sumFScore=0\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "validationResults=crossValidate(trainData,10)\n",
    "for i in range(len(validationResults)):\n",
    "    print('Fold ' + str(i+1) + ': \\nPrecision: %f\\tRecall: %f\\tF Score:%f' % validationResults[i][:3])\n",
    "    sumFScore+=validationResults[i][2]\n",
    "print('Average FScore:%f' % (sumFScore/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can see the results for each fold. If we take out an average of each FScore and the accuracy it comes around to 0.63. This is not a good model. We need to tweak some things in order to improve score. That is done in Q4 & 5 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'this': 1, 'assortment': 1, 'is': 1, 'really': 1, \"hershey's\": 1, 'at': 1, 'their': 1, 'best.': 1, 'the': 2, 'little': 1, 'ones': 1, 'are': 1, 'always': 1, 'excited': 1, 'whenever': 1, 'holidays': 1, 'come': 1, 'because': 1, 'of': 1, 'this.': 1}, 'fake')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.625260\n",
      "Recall: 0.625238\n",
      "F Score:0.625221\n",
      "Accuracy: 0.625238\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    print(\"Accuracy: %f\" % accuracy_score(testTrue, testPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can see that the FScore and the Average is a bit low. We need to improve that. Q4 & 5 notebook does the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 4 and 5\n",
    "Once you're happy with your functions for Questions 1 to 3, it's advisable you make a copy of this notebook to make a new notebook, and then within it adapt and improve all three functions in the ways asked for in questions 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements of all functions is done in a separate notebook with title \"Atharva Joshi 200425197 Q4,5\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
