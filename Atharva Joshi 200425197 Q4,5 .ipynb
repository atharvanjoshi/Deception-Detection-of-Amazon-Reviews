{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is in continuation with the previous notebook titled \"Atharva Joshi 200425197 Q1,2,3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path,encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            (Id, Text,AmazonCategory,AmazonRating,AmazonVerifiedReview,Label) = parseReview(line)\n",
    "            rawData.append((Id, Text,AmazonCategory,AmazonRating,AmazonVerifiedReview,Label))\n",
    "\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, AmazonCategory, AmazonRating, AmazonVerifiedReview, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text),AmazonCategory,AmazonRating,AmazonVerifiedReview),Label))\n",
    "    for (_, Text, AmazonCategory, AmazonRating, AmazonVerifiedReview, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text),AmazonCategory,AmazonRating,AmazonVerifiedReview),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous notebook we saw that FScore was a bit low. We will try to improve that. First we will try to add more features. Earlier we used only text to predict. We will add some supporting features to influence its decision. Here we will add three more features Category, Rating and check if it's a verified review. Category will tell us whether a specific category has more fake reviews. eg. A mobile phone company might plant fake positive reviews about their products to influence people. Hence if such a category is detected, it will be an important parameter in the decision. Rating will help such thata huge number of 5 stars or 1 star may indicate paid positive or negative reviews. We need to filter out such fake reviews. Verified Review helps us understand whether the reviewer has actually used the product. Such verified reviewers are likely to give a genuine review. We also tried Product ID and Product Title instead of Category and Rating and got average FScore of 0.537 and an average accuracy of 0.524 which is much less. That makes sense since ID and Title don't really contribute to the decision of the review being real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    if reviewLine[1]=='__label2__':\n",
    "        reviewLine[1]=realLabel\n",
    "    else:\n",
    "        reviewLine[1]=fakeLabel\n",
    "    return (reviewLine[0], reviewLine[8], reviewLine[4],reviewLine[2],reviewLine[3],reviewLine[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earlier we saw that the data at specific labels was returned. It is now tweaked to accomodate the three new features that we took. Category, Rating and Verified Review are at 4, 2 and 3 positions respectively. Here we also assigned \"\\_\\_label2\\_\\_\" as \"Real\" and \"\\_\\_label1\\_\\_\" as \"Fake\" for easy understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import re,nltk,string\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    \n",
    "    #Remove @tags\n",
    "    TAG_CLEANING_RE = \"@\\S+\"\n",
    "    text=re.sub(TAG_CLEANING_RE, ' ', text)\n",
    "    \n",
    "    #Removing website links\n",
    "    LINKS_CLEANING_RE = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "    text=re.sub(LINKS_CLEANING_RE, ' ', text)\n",
    "    \n",
    "    #Remove Punctuation\n",
    "    text=text.translate(text.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #remove white spaces\n",
    "    text=text.strip()\n",
    "    \n",
    "    #Convert To Lower Case\n",
    "    text=text.lower()\n",
    "    \n",
    "    #Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    #Stop Words removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    #Porter Stemming\n",
    "    #stemmer= PorterStemmer()\n",
    "    #tokens = [stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    #Lemmatiztion\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    \n",
    "    #ngrams = zip(*[tokens[i:] for i in range(2)])\n",
    "    #tokens= [\" \".join(ngram) for ngram in ngrams]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous notebook we mentioned that there are some problems with the tokenisation process like the number of tokens in this was way too large. These unnecessary tokens can be easily removed. We do that by doing the following:\n",
    "### 1) @ Tags:\n",
    "### Some users mention other users or the seller in their reviews. This doesn't affect the result and is an unnecessary addition to the corpus. Hence we use Regular Expression to remove such tags. Without this we get an average Fscore of 0.776154 and Average Accuracy of 0.776154. \n",
    "### 2) Website Links:\n",
    "### Some users might use website links maybe to suggest similar and/or better and/or cheaper products to other users. We remove those too.  Without this we get an average Fscore of 0.772114 and Average Accuracy of 0.775528. \n",
    "### 3) Punctuation\n",
    "### eg. \"I am happy.\" will be tokenized into ['I','am','happy','.']. Here there is a full stop which doesn't matter for the decision but is unnecessarily increases the vector space. Hence we remove all the punctuations.  Without this we get an average Fscore of 0.772178 and Average Accuracy of 0.773148. \n",
    "### 4) Lower Case\n",
    "### eg. {\"I am Happy\"}, {\"I am happy\"}. These two sentences will be tokenised as ['I','am','Happy','happy']. Here we know that Happy=happy. Hence to avoid this we convert every letter in the text to lower form.  Without this we get an average Fscore of 0.771459 and Average Accuracy of 0.772156. \n",
    "### 5) Remove white spaces.\n",
    "### Since we are splitting on white spaces, we need to make sure there isn't any before and after a text. Strip removes all unnecessary white spaces before and after the text.  This didn't make any difference to FScore and Accuracy\n",
    "### 6) Tokenisation\n",
    "### Here we use a library 'NLTK' to tokenize the text by splitting on white spaces.\n",
    "### 7) Stop words removal\n",
    "### Some examples of stopwords are \"a,\" \"and\", \"but\", \"how\", \"or\" and \"what.\" These words are unnecessary for the text to be classified and have probably a large count since they are used in all sentences thus overshadowing other important words (Remember Language is Zipfian).  Without this we get an average Fscore of 0.775621 and Average Accuracy of 0.775543. \n",
    "### 8) Stemming\n",
    "### eg. Lion and Lions. Here the plurality of the lion doesn't matter in the context. Stemming brings words to its basic form like in this case Lions becomes Lion to establish equality. In this case we have used Porter Stemmer which is widely used. \n",
    "### 9) Lemmatization\n",
    "### In lemmatization, each words is brought to it's basic root form. eg. apple/apples becomes appl and berry/berries becomes berri. This helps establish equality between all the words having same roots but different forms.\n",
    "### Note: Lemmatization in this case performs better than stemming.  With Stemming we get an average Fscore of 0.772320 and Average Accuracy of 0.772187. Using both doesn't make any difference.\n",
    "### 10) Bigrams\n",
    "### Till now we divided the text into single tokens called unigrams. Here we tried to use bigrams. Bigrams establish relationship with the previous word thus facilitating better prediction. We also tried higher orders like trigrams and 4-grams but the result was pretty much similar. However, unigrams still gave a slight better result than other n-grams which gave average Fscore of 0.780357 and Average Accuracy of 0.779553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens,AmazonCategory,AmazonRating,AmazonVerifiedReview):\n",
    "    featureDictLocal={}\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            featureDict[t] += 1\n",
    "            featureDictLocal[t] += 1\n",
    "        except KeyError:            \n",
    "            featureDict[t] = 1\n",
    "            featureDictLocal[t] = 1\n",
    "    featureDict.update({'AmazonCategory':AmazonCategory,'AmazonRating':AmazonRating,'AmazonVerifiedReview':AmazonVerifiedReview})\n",
    "    featureDictLocal.update({'AmazonCategory':AmazonCategory,'AmazonRating':AmazonRating,'AmazonVerifiedReview':AmazonVerifiedReview})\n",
    "    return featureDictLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earlier we assigned weights based on the frequency of the words appearing in the text. Here we assign the weights by dividing them with the total length of the dataset. The weight increases as the count of words increases but it's small enough that it doesn't overflow. But this method gave a lower average Fscore of 0.765648 and Average Accuracy of 0.764412. Even here we are maintaining a global and a local dictionary. We also used other methods like CountVectorizer or TF-IDF Vectorizer which are available in different libraries but gave a similar performance. Hence we reverted back to the frequency vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(penalty='l2',max_iter=2000, loss='hinge',dual=True,random_state=100,\n",
    "                                            verbose=1,C=0.001,class_weight='balanced', fit_intercept=True,\n",
    "                                            intercept_scaling=1,multi_class='ovr'))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC is the SVM we're using to train the classifier. Certain parameters can be tweaked to optimise it's performance and converge to a minima in the loss function. Following was observed during the tweaking of parameters:\n",
    "### 1)Penalty default is L2. L1 leads to coef_ that are sparse. Since L1 is not supported with hinge or squared_hinge it gave an error and hence not used.\n",
    "### 2)tol default value 1e-3 is suitable. Increasing or decreasing reduces average f score and average accuracy to around 0.77 both.\n",
    "### 3)Max inter from 1k to 2k because of the Convergence Warning we encountered during training. No change in FScore or average\n",
    "### 4)Loss Function Hinge gave slightly higher f1 score. (0.77 v/s 0.78)\n",
    "### 5)Dual is true(default). Should be false only when n_samples>n_features. So did not change.\n",
    "### 6)Random state has no effect on FScore and Accuracy\n",
    "### 7)Verbose has no effect. Only used to print more related metadata.\n",
    "### 8)C is the Regularization parameter. Strictly should be positive. Default is 1.0 and decreasing increases the Fscore and Average slightly than 0.76 which was obtained by increasing.\n",
    "### 9)class_weight: The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). Default is None where all features have weight 'one'. Didn't make any difference in the score.\n",
    "### 11)Default values are taken for fir_intercept and intercept_scaling. Doesn't affect Fscore.\n",
    "### 12)Changing multi_class to 'crammer_singer' has extremely slight decrease in fscore to around 0.74. Hence kept default i.e. 'ovr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "crossValidationActual=[]\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    accuracySum = 0\n",
    "    accuracyAverage = 0.0\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        crossValidationTestData = dataset[i:i+foldSize]\n",
    "        crossValidationTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(crossValidationTrainData)\n",
    "        crossValidationActual = [x[1] for x in crossValidationTestData]\n",
    "        crossValidationPredictedLabels = predictLabels(crossValidationTestData,classifier)\n",
    "        cv_results.append(precision_recall_fscore_support(crossValidationActual, crossValidationPredictedLabels, average='weighted'))\n",
    "        accuracySum+=(accuracy_score(crossValidationActual, crossValidationPredictedLabels))\n",
    "    print('\\nAverage Accuracy:%f' % (accuracySum/10))\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm and implementation remains same here. We can also use the libraries to do K Fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "30324\n",
      "Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]\n",
      "Average Accuracy:0.782857\n",
      "Fold 1: \n",
      "Precision: 0.783885\tRecall: 0.775595\tF Score:0.774163\n",
      "Fold 2: \n",
      "Precision: 0.782435\tRecall: 0.778571\tF Score:0.777937\n",
      "Fold 3: \n",
      "Precision: 0.795714\tRecall: 0.791667\tF Score:0.790222\n",
      "Fold 4: \n",
      "Precision: 0.780969\tRecall: 0.780357\tF Score:0.780101\n",
      "Fold 5: \n",
      "Precision: 0.800612\tRecall: 0.793452\tF Score:0.792266\n",
      "Fold 6: \n",
      "Precision: 0.787950\tRecall: 0.784524\tF Score:0.784169\n",
      "Fold 7: \n",
      "Precision: 0.794007\tRecall: 0.789881\tF Score:0.788933\n",
      "Fold 8: \n",
      "Precision: 0.777702\tRecall: 0.767262\tF Score:0.766176\n",
      "Fold 9: \n",
      "Precision: 0.788465\tRecall: 0.785714\tF Score:0.784908\n",
      "Fold 10: \n",
      "Precision: 0.785983\tRecall: 0.781548\tF Score:0.780913\n",
      "Average FScore:0.781979\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "sumFScore=0\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "validationResults=crossValidate(trainData,10)\n",
    "for i in range(len(validationResults)):\n",
    "    print('Fold ' + str(i+1) + ': \\nPrecision: %f\\tRecall: %f\\tF Score:%f' % validationResults[i][:3])\n",
    "    sumFScore+=validationResults[i][2]\n",
    "print('Average FScore:%f' % (sumFScore/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can see the results for each fold. After all the operations, the approximate average of FScore is around 0.78 and the Average Accuracy is also 0.78 which is significantly better than what we had in the previous notebook. Hence, our ways to improve the methods to optimise the performance worked properly. Here we can see that the number of features has also significantly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'assortment': 1, 'really': 1, 'hershey': 1, 'best': 1, 'little': 1, 'one': 1, 'always': 1, 'excited': 1, 'whenever': 1, 'holiday': 1, 'come': 1, 'AmazonCategory': 'Grocery', 'AmazonRating': '5', 'AmazonVerifiedReview': 'N'}, 'fake')\n",
      "Training Classifier...\n",
      "[LibLinear]Done training!\n",
      "Precision: 0.815233\n",
      "Recall: 0.808095\n",
      "F Score:0.807003\n",
      "Accuracy: 0.808095\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    print(\"Accuracy: %f\" % accuracy_score(testTrue, testPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can say that there is significant improvement in the FScore and the Accuracy compared to the previous notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
